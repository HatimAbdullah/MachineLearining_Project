{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disicion tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bugs = pd.read_csv('Eclipse_ JDT_Core_single-version-ck-oo_bugs_only.csv',error_bad_lines=False)\n",
    "\n",
    "bugs_Eclipse_JDT = pd.read_csv('Eclipse_ JDT_Core_single-version-ck-oo_bugs_only.csv', error_bad_lines=False)\n",
    "bugs_Eclipse_PDE = pd.read_csv('Eclipse_PDE_UI_single-version-ck-oo_bug_only.csv', error_bad_lines=False)\n",
    "bugs_Equinox = pd.read_csv('Equinox_Framework_single-version-ck-oo_bug_only.csv', error_bad_lines=False)\n",
    "bugs_Lucene = pd.read_csv('Lucene_single-version-ck-oo_bug_only.csv', error_bad_lines=False)\n",
    "bugs_Mylyn = pd.read_csv('Mylyn_single-version-ck-oo_bug_only.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs[' bugs '] = bugs[' bugs '].apply(lambda x : +1 if x==0 else -1)\n",
    "\n",
    "bugs_Eclipse_JDT[' bugs '] = bugs_Eclipse_JDT[' bugs '].apply(lambda x : +1 if x==0 else -1)\n",
    "bugs_Eclipse_PDE[' bugs '] = bugs_Eclipse_PDE[' bugs '].apply(lambda x : +1 if x==0 else -1)\n",
    "bugs_Equinox[' bugs '] = bugs_Equinox[' bugs '].apply(lambda x : +1 if x==0 else -1)\n",
    "bugs_Lucene[' bugs '] = bugs_Lucene[' bugs '].apply(lambda x : +1 if x==0 else -1)\n",
    "bugs_Mylyn[' bugs '] = bugs_Mylyn[' bugs '].apply(lambda x : +1 if x==0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    " ' cbo ',\n",
    " ' dit ',\n",
    " ' fanIn ',\n",
    " ' fanOut ',\n",
    " ' lcom ',\n",
    " ' noc ',\n",
    " ' numberOfAttributes ',\n",
    " ' numberOfAttributesInherited ',\n",
    " ' numberOfLinesOfCode ',\n",
    " ' numberOfMethods ',\n",
    " ' numberOfMethodsInherited ',\n",
    " ' numberOfPrivateAttributes ',\n",
    " ' numberOfPrivateMethods ',\n",
    " ' numberOfPublicAttributes ',\n",
    " ' numberOfPublicMethods ',\n",
    " ' rfc ',\n",
    " ' wmc ']\n",
    "target = ' bugs '\n",
    "bugs = bugs[features + [target]]\n",
    "\n",
    "bugs_Eclipse_JDT = bugs_Eclipse_JDT[features + [target]]\n",
    "bugs_Eclipse_PDE = bugs_Eclipse_PDE[features + [target]]\n",
    "bugs_Equinox = bugs_Equinox[features + [target]]\n",
    "bugs_Lucene = bugs_Lucene[features + [target]]\n",
    "bugs_Mylyn = bugs_Mylyn[features + [target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs = pd.get_dummies(bugs, columns=features, prefix_sep='.', dummy_na=False).fillna(0)\n",
    "\n",
    "bugs_Eclipse_JDT = pd.get_dummies(bugs_Eclipse_JDT, columns=features, prefix_sep='.', dummy_na=False).fillna(0)\n",
    "bugs_Eclipse_PDE = pd.get_dummies(bugs_Eclipse_PDE, columns=features, prefix_sep='.', dummy_na=False).fillna(0)\n",
    "bugs_Equinox = pd.get_dummies(bugs_Equinox, columns=features, prefix_sep='.', dummy_na=False).fillna(0)\n",
    "bugs_Lucene = pd.get_dummies(bugs_Lucene, columns=features, prefix_sep='.', dummy_na=False).fillna(0)\n",
    "bugs_Mylyn = pd.get_dummies(bugs_Mylyn, columns=features, prefix_sep='.', dummy_na=False).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features (after binarizing categorical variables) =  1293\n"
     ]
    }
   ],
   "source": [
    "features1 = bugs_Eclipse_JDT.columns\n",
    "features2 = bugs_Eclipse_PDE.columns\n",
    "features3 = bugs_Equinox.columns\n",
    "features4 = bugs_Lucene.columns\n",
    "features5 = bugs_Mylyn.columns\n",
    "features\n",
    "print(\"Number of features (after binarizing categorical variables) = \", len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(701, 1666) (149, 1666)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "msk = np.random.rand(len(bugs)) < 0.7 # a mask with 80% True\n",
    "\n",
    "train_data, test_data = bugs[msk], bugs[~msk]\n",
    "\n",
    "msk2=np.random.rand(len(test_data)) < 0.5\n",
    "\n",
    "val_data, test_data = test_data[msk2], test_data[~msk2]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(701, 1666) (149, 1666)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "#########################################\n",
    "mskA = np.random.rand(len(bugs_Eclipse_JDT)) < 0.7\n",
    "\n",
    "train_data_1, test_data_1 = bugs_Eclipse_JDT[mskA], bugs_Eclipse_JDT[~mskA]\n",
    "\n",
    "mskA2=np.random.rand(len(test_data_1)) < 0.5\n",
    "\n",
    "val_data_1, test_data_1 = test_data_1[mskA2], test_data_1[~mskA2]\n",
    "\n",
    "#########################################\n",
    "mskB = np.random.rand(len(bugs_Eclipse_PDE)) < 0.7\n",
    "\n",
    "train_data_2, test_data_2 = bugs_Eclipse_PDE[mskB], bugs_Eclipse_PDE[~mskB]\n",
    "\n",
    "mskB2=np.random.rand(len(test_data_2)) < 0.5\n",
    "\n",
    "val_data_2, test_data_2 = test_data_2[mskB2], test_data_2[~mskB2]\n",
    "\n",
    "#########################################\n",
    "mskC = np.random.rand(len(bugs_Equinox)) < 0.7\n",
    "\n",
    "train_data_3, test_data_3 = bugs_Equinox[mskC], bugs_Equinox[~mskC]\n",
    "\n",
    "mskC2=np.random.rand(len(test_data_3)) < 0.5\n",
    "\n",
    "val_data_3, test_data_3 = test_data_3[mskC2], test_data_3[~mskC2]\n",
    "\n",
    "#########################################\n",
    "mskD = np.random.rand(len(bugs_Lucene)) < 0.7\n",
    "\n",
    "train_data_4, test_data_4 = bugs_Lucene[mskD], bugs_Lucene[~mskD]\n",
    "\n",
    "mskD2=np.random.rand(len(test_data_4)) < 0.5\n",
    "\n",
    "val_data_4, test_data_4 = test_data_4[mskD2], test_data_4[~mskD2]\n",
    "\n",
    "#########################################\n",
    "mskE = np.random.rand(len(bugs_Mylyn)) < 0.7\n",
    "\n",
    "train_data_5, test_data_5 = bugs_Mylyn[mskE], bugs_Mylyn[~mskE]\n",
    "\n",
    "mskE2=np.random.rand(len(test_data_5)) < 0.5\n",
    "\n",
    "val_data_5, test_data_5 = test_data_5[mskE2], test_data_5[~mskE2]\n",
    "\n",
    "print(train_data_1.shape, test_data_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "    bugs = (labels_in_node==1).sum()\n",
    "    \n",
    "\n",
    "    no_bugs = (labels_in_node==-1).sum()\n",
    "           \n",
    "\n",
    "    return  min(bugs, no_bugs)\n",
    "\n",
    "def best_splitting_feature(data, features, target):\n",
    "    \n",
    "    best_feature = None \n",
    "    best_error = 10    \n",
    "    \n",
    "\n",
    "\n",
    "    num_data_points = float(len(data))  \n",
    "    \n",
    "   \n",
    "    for feature in features:\n",
    "        \n",
    "\n",
    "        left_split = data[data[feature] == 0]\n",
    "       \n",
    "        right_split = data[data[feature] == 1] \n",
    "            \n",
    "        \n",
    "        left_mistakes = intermediate_node_num_mistakes(left_split[target])           \n",
    "\n",
    "\n",
    "        right_mistakes = intermediate_node_num_mistakes(right_split[target]) \n",
    "            \n",
    "       \n",
    "        error = (left_mistakes+right_mistakes)/num_data_points\n",
    "\n",
    "        if error >= best_error:\n",
    "             best_feature=best_feature\n",
    "        else:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    return best_feature \n",
    "\n",
    "def create_leaf(target_values):\n",
    "        leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True }  \n",
    "    \n",
    "\n",
    "        num_ones = len(target_values[target_values == 1])\n",
    "        num_minus_ones = len(target_values[target_values == -1])\n",
    "    \n",
    "\n",
    "        if num_ones > num_minus_ones:\n",
    "            leaf['prediction'] = 1       \n",
    "        else:\n",
    "            leaf['prediction'] = -1         \n",
    "\n",
    "    \n",
    "        return leaf \n",
    "def decision_tree_create(data, features, target, current_depth = 0, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    \n",
    "    target_values = data[target]\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "    \n",
    "\n",
    "\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:  \n",
    "\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "\n",
    "    if remaining_features.any() == 'None': \n",
    "        print(\"Stopping condition 2 reached.\"    )\n",
    "\n",
    "        return create_leaf(target_values)    \n",
    "    \n",
    "\n",
    "    if current_depth >=max_depth :  \n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "\n",
    "        return create_leaf(target_values)\n",
    "\n",
    "\n",
    "    splitting_feature=best_splitting_feature(data, features, target)\n",
    "\n",
    "\n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]     \n",
    "    remaining_features = remaining_features.drop(splitting_feature)\n",
    "    print(\"Split on feature\" ,splitting_feature,\".(\",len(left_split),\",\",len(right_split),\")\")   \n",
    "\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target])\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(right_split[target])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, current_depth + 1, max_depth)        \n",
    "\n",
    "    right_tree = decision_tree_create(right_split, remaining_features, target, current_depth + 1, max_depth)  \n",
    "\n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}\n",
    "\n",
    "def classify(tree, x, annotate = False):   \n",
    "\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print(\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)\n",
    "def evaluate_classification_error(tree, data, target):\n",
    "    prediction = []\n",
    "    for i in range(len(data)):\n",
    "        prediction.append(classify(tree, data.iloc[i]))\n",
    "        \n",
    "\n",
    "    mistakes = 0\n",
    "    for i in range(len(data)):\n",
    "        if prediction[i] != data.iloc[i][target]:\n",
    "            mistakes = mistakes + 1\n",
    "    return mistakes/len(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (701 data points).\n",
      "Split on feature  bugs  .( 0 , 557 )\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (0 data points).\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (557 data points).\n"
     ]
    }
   ],
   "source": [
    "decision_tree_1 = decision_tree_create(train_data_1, features1, ' bugs ', max_depth = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2214765100671141"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(decision_tree_1, test_data_1, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1055 data points).\n",
      "Split on feature  bugs  .( 0 , 908 )\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (0 data points).\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (908 data points).\n"
     ]
    }
   ],
   "source": [
    "decision_tree_2 = decision_tree_create(train_data_2, features2, ' bugs ', max_depth = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1318181818181818"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(decision_tree_2, test_data_2, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (224 data points).\n",
      "Split on feature  bugs  .( 0 , 131 )\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (0 data points).\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (131 data points).\n"
     ]
    }
   ],
   "source": [
    "decision_tree_3 = decision_tree_create(train_data_3, features3, ' bugs ', max_depth = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3111111111111111"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(decision_tree_3, test_data_3, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (476 data points).\n",
      "Split on feature  bugs  .( 0 , 439 )\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (0 data points).\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (439 data points).\n"
     ]
    }
   ],
   "source": [
    "decision_tree_4 = decision_tree_create(train_data_4, features4, ' bugs ', max_depth = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09574468085106383"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(decision_tree_4, test_data_4, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (1297 data points).\n",
      "Split on feature  bugs  .( 0 , 1122 )\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (0 data points).\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (1122 data points).\n"
     ]
    }
   ],
   "source": [
    "decision_tree_5 = decision_tree_create(train_data_5, features5, ' bugs ', max_depth = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10305343511450382"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(decision_tree_5, test_data_5, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
